---
title: "Snapshot Wisconsin data pull workflow using `sswidb` and `sswids`"
date: "`r format(Sys.time(), '%B %d %Y')`"
output: 
  rmarkdown::html_vignette:
    toc: true
    
vignette: >
  %\VignetteIndexEntry{DataPullTutorial}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup, include = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(readr)
library(tidyr)
library(ggplot2)
library(sswidb)
library(sswids)
library(sf)
library(mapview)
library(kableExtra)
#library(raster)
#library(landscapemetrics)
#library(exactextractr)
#library(nngeo)
select <- dplyr::select
sswids::connect_to_sswidb(db_version = 'PROD')


cam_distance = 100

# read in demo files
effort_df_camsiteid_demo <- read_rds('data_demo/effort_df_camsiteid.Rds')
effort_by_day_demo <- read_rds('data_demo/effort_by_day_df.Rds')
effort_by_occ_demo <- read_rds('data_demo/effort_by_occ_df.Rds')
detections_maxcount_demo <- read_rds('data_demo/detections_wide_df_maxcount.Rds')
detections_counttriggers_demo <- read_rds('data_demo/detections_wide_df_counttriggers.Rds')
dets_all_plot_demo <- read_csv <- read_rds('data_demo/dets_all_plot.Rds')
locs_df_removelowpreclocs_demo <- read_rds('data_demo/locs_df_removelowpreclocs.Rds')
locs_df_camsiteid_demo <- read_rds('data_demo/locs_df_camsiteid.Rds')
mean_locs_demo <- read_rds('data_demo/mean_locs_df.Rds')
mean_locs_sf_layers_demo <- read_rds('data_demo/mean_locs_sf_layers.rds')

# read in raw data files
detections_df_raw <- read_rds('data_demo/detections_df_raw.Rds')
effort_df_raw <- read_rds('data_demo/effort_df_raw.Rds')
locs_df_raw <- read_rds('data_demo/locations_df_raw.rds')

# read in clean data files
effort_maxcount <- read_rds('data_demo/effort_by_occ_df_maxcount_join1000.Rds')
effort_counttriggers <- read_rds('data_demo/effort_by_occ_df_counttriggers_join1000.Rds')

```

# Background and setup

`sswids` builds on the `sswidb` package (thanks Ryan!) and provides a set of functions for the Snapshot Wisconsin Decision Support Team to query, clean, and format Snapshot Wisconsin data in a standardized way for our team's analyses and products. This tutorial walks through setting up the `sswids` package, querying the raw data, cleaning it, and generating encounter histories.
\

## Installation

Many of the functions in the `sswids` package are wrappers for Ryan's `sswidb` functions, so we first need to install `sswidb`. `sswidb` needs to be installed from the most recent `.tar.gz` file located [here](//central/OAS/Snapshot/SnapshotRpackage/sswidb/in_dev). You can either set the path to the `sswidb` file yourself, or use `file_choose()` inside of `install.packages()` to navigate to it.

```{r install1, eval = FALSE}
install.packages(file.choose(), source = TRUE, repos = NULL)
```
\

Then `sswids` can be installed from the GitHub repository (R will likely ask for you to install or update several dependencies).

```{r install2, eval = FALSE}
devtools::install_github("SnapshotWisconsin/sswids")
```
\

Check package versions of `sswidb` and `sswids`.

```{r package_versions, message=FALSE,warning=FALSE}

packageVersion("sswidb")
packageVersion("sswids")

```
\

Once the necessary R packages are installed, we can load `sswidb` and `sswids`. We also need to load the `tidyverse` and `sf` packages to help with data cleaning and spatial operations.

```{r libraries, eval = FALSE}
library(tidyverse)
library(sswidb)
library(sswids)
library(sf)
library(mapview)
#library(raster)
#library(landscapemetrics)
#library(exactextractr)
#library(nngeo)
select <- dplyr::select
```
\

## Connecting to Snapshot Wisconsin database

`sswids` includes a function to set your Snapshot Wisconsin data base credentials once, then automatically access the data base in subsequent sessions using the `connect_to_sswidb` function (rather than storing the `.yaml` credentials file locally on your machine). If you're trying to connect for the first time you can store your credentials with `establish_sswidb_credentials`. You should see three windows pop up to enter the dsn, uid, and pwd from the `.yaml` file, which are then stored in your machine's keychain.

You also have the option of connecting to the 'production' (PROD) data base, or the 'testing' (UAT) data base. Each of these requires a different set of credentials to connect to them.

```{r credentials, eval = FALSE}
# store sswidb credentials; only need to do this once
sswids::establish_sswidb_credentials(db_version = 'PROD')
```
\

Once your credentials are saved in the keychain, you can skip the step above and just connect directly to either the PROD or UAT data base. In most instances we'll be using the PROD data base.

Finally, you can set up folders in your project directory with `setup_project` to store raw data, cleaned data, model outputs, data checks, etc. This will create 3 folders: `data_clean`, `data_raw`, and `output`.

```{r connect, eval = FALSE}
# connect to data base
sswids::connect_to_sswidb(db_version = 'PROD')

# create project folders
sswids::setup_project()
```
\

## Set data parameters

Now that the R packages are installed and loaded, we need to set several parameters for querying, cleaning, and formatting the data. `sswids` is set up to query data within a 'season', or within date ranges across multiple years (e.g., querying data in August of each year to calculate turkey reproductive metrics). `sswids` includes a helper function, `create_season_dates` to do this for you automatically as long as you provide the following information:

- **Start/end year** - Years to query across (these can be the same if you'd like to look at a single year)
- **Start/end date** - Season start and end date. Date ranges must be entered in -MM-DD format

`create_season_dates` (and the subsequent workflow) can handle instances where a season spans two calendar years (i.e., querying data in winter months). For example, if a season is defined as the months of December and January, then the 2018 season includes December of 2018 and January of 2019. Start/end dates can also be set to the first and last day of the year to query data across every day for a range of years.

Date time quality control (DTQC) has been run for SSWI data back to 2018-01-01. Therefore, be cautious with querying for data prior to the start of 2018 as there may be more date time issues. 

In this example, we define our season as September 1 - October 31, and we'd like to query data from each season in 2018, 2019, 2020, 2021, 2022, and 2023:

```{r season, eval = TRUE}
# set min/max years for obtaining data
min_year <- 2018
max_year <- 2023

# season start/end dates (in -MM-DD format)
min_date <- '-07-01'
max_date <- '-08-31'

# what years of data to query?
years <- seq(min_year, max_year, 1)

# create data frame of seasons for data filtering
seasons_df <-
  create_season_dates(
    min_date = min_date,
    max_date = max_date,
    years = years
  )

# check out these dates
sswids::check_season_dates(seasons_df)
```
\

Next, we need to decide which species and grid types we'd like to include. The `sswidb` package has functions for printing a table to display codes for each of these.

Species:
\

```{r species, eval = FALSE}
# check sswidb species names
sswidb_species(conn)
```

```{r species_tbl, echo = FALSE}
# check sswidb species names
sswidb_species(conn) %>%
  kableExtra::kbl() %>%
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover")) %>%
  kableExtra::scroll_box(width = "100%", height = "300px")
```
\

Grid:
\

```{r grid, eval = FALSE}
# check sswidb grid types
sswidb_grid_types(conn)
```

```{r grid_tbl, echo = FALSE}
# check sswidb grid types
sswidb_grid_types(conn) %>%
  kableExtra::kbl() %>%
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover"))
```
\

We can then set one or more species and grid types. We'll look at deer within the SSWI and Elk grids here. Choose a less abundant species, shorter time frame, or select counties to run this query more quickly. 

```{r species_grid_set, eval = FALSE}
# set species and grid types
species <- c('Grouse')
grid <- c('SSWI','ELKBR')
```
\

At this point we should also set a few other variables that are needed for querying the raw data or cleaning/formatting the data in later steps:

- **Classification precision** - this number determines whether certain volunteer detections are returned or not based on the Zooniverse/MySnapshot accuracy levels. It had been standard to use 0.95 so that only classifications where the lower 95% credible interval is above 0.95 are returned. However, it is difficult to assess the effect of a certain precision threshold unless all classifications are included at this level. Therefore, use `prec = 0` to include all classifications.

- **Camera distance** - if cameras are within this distance (in meters) they are merged and considered the same camera site. Use `cam_distance = 100` to merge camera locations that are within 100 m of each other.

- **Coordinate precision** - number of digits after the decimal place of the latitude/longitude coordinates. Use `coord_prec = 4` to include all locations where the location is precise to within 11 m (4 decimal places) of the coordinates. Since summer 2023, any new camera locations require at least 4 decimal places so this filter is only for locations that were entered prior to that date.

- **Proportion classified threshold** - the proportion of triggers that have been classified within a sampling occasion (if the proportion of triggers classified is less than this number then counts are converted to NA values). In order to assess the data loss at this step, use `ppn_class_threshold = 0` to include all data regardless of proportion classified. 

- **Number of sampling occasions** - number of sampling occasions used when building encounter histories. To get most granular data (i.e., by day), use the number of sampling occasions equal to the number of days in the sampling season. However, this gets tricky when sampling February over multiple years because of leap year. For this example, use `num_occasions = 10`, though this does not divide evenly for our season which is 61 days. 

ASSESS:
*At this point in time, each sampling occasion must be the same length (e.g., 6 days), which requires a season length which is a nice round number*

```{r other_params, eval = FALSE}
# set classification precision level
prec <- 0

# set distance (meters) between camera_location_seq_nos to merge/average locations
cam_distance <- 100

# set precision of lat/long coordinates (how many decimal places are needed?)
coord_prec <- 4

# set desired proportion of photos classified (per sampling occasion) threshold
ppn_class_threshold <- 0

```
\

## Query raw data

Now that all of our data parameters have been set, we can query the necessary raw data from the Snapshot Wisconsin data base using `sswids::query_raw_data()`. The resulting object, `raw_data`, contains a list of data frames for each separate query (detections, effort, and locations).

```{r raw_query, eval = FALSE}
# query databases
raw_data <-
  sswids::query_raw_data(
    species = species,
    grid = grid,
    season = seasons_df, # season dates/years to query
    prec = prec # ,
    # by default all 3 data sets are queried, but can just query detections/effort only 
    # outputs = c('detections', 'effort', 'locations')
  )
```

Once the data base queries are complete, we should save them to have a record of the raw data. This also allows us to run the query above just once. `sswids::write_raw_data()` now has an optional argument `filename` that allows you to append a character string to the file name to help identify the data and not overwrite previously downloaded data. For example I saved `detections_df_rawALL.csv` to denote this .csv has data from the entire Snapshot time frame as opposed to a smaller season of data.

```{r write_raw, eval = FALSE}
# save the raw queries
write_raw_data(raw_data)
```

To continue cleaning and prepping the data, we need to split the `raw_data` object into 3 data frames: detection, effort, and locations.

```{r read_raw, eval = FALSE}
# continue using detections, effort, and locations separately
# read in from raw_data folder
detections_df_raw <- read_csv('./data_raw/detections_df_raw.csv')
effort_df_raw <- read_csv('./data_raw/effort_df_raw.csv')
locs_df_raw <- read_rds('./data_raw/locations_df_raw.rds')
```

Here are the first 1000 rows from each of these three data frames:

```{r detections_tbl, echo = FALSE}
detections_df_raw[1:1000,] %>%
  kableExtra::kbl(caption="Detections raw data") %>%
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover")) %>%
  kableExtra::scroll_box(width = "100%", height = "300px")
```
\ 

```{r effort_tbl, echo = FALSE}
effort_df_raw[1:1000,] %>%
  kableExtra::kbl(caption="Effort raw data") %>%
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover")) %>%
  kableExtra::scroll_box(width = "100%", height = "300px")
```
\

```{r locations_tbl, echo = FALSE}
locs_df_raw[1:1000,] %>%
  kableExtra::kbl(caption="Locations raw data") %>%
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover")) %>%
  kableExtra::scroll_box(width = "100%", height = "300px")
```
\
\

# Data cleaning

## Remove detections without Wisconsin location data

There are detections that have no effort or location data. This is likely because of detection date times outside of the `camera_location_seq_no` start and end dates. Find these and remove them. 

Also, there is 1 location that falls in the UP of Michigan. Remove this location as well. This is an old location from 2020 and earlier, so it cannot be corrected. The spatial data, and zone arguments will depend on the data you're working on, for this example...

```{r rm_noloc_data, eval=FALSE}
list_nolocs <- rm_noloc_data(locs_df = locs_df_raw, 
                             effort_df = effort_df_raw, 
                             detections_df = detections_df_raw, 
                             spatial_data = "ruffed_grouse_priority_areas", 
                             zone = rugr_pr)
```


The "spatial data" argument is filled by the name of a layer returned from `list_spatial_layers()`, notice it is provided as a character string. The "zone" argument specifies the column in the spatial data relating to zone, management unit or similar spatial unit, specified without quotes! The default arguments are to load the "counties" layer with county_name as the zone argument. See `?rm_noloc_data` for more help if needed.

## Remove impossible dates

There are detections with dates outside of the start and end active dates on a batch. These must be errors and should be corrected in the database. 

At least some of these are errors because of the location start date not overlapping with the batch start and end dates. The batch `START_ACTIVE_DATE` and `END_ACTIVE_DATE` and the location `START_DATE` and `END_DATE` are both assessed when determining `start_date` and `end_date` for effort from the `sswidb_effort` function. 

Find all batches that have detections outside of batch start and end active dates and remove those batches and all associated detections and locations. 

```{r remove_bad_batches, eval=FALSE}
list_outsideactivedates <- rm_bad_batches(datalist=list_nolocs)
```

## Remove low precision locations

Until summer 2023, there was no requirement for the number of decimal places when recording latitude or longitude coordinates, or the ability to record trailing zeros in the database. Therefore, to ensure precise location data, we keep `camera_location_seq_nos` in the locations data frame that have a latitude or longitude coordinate with at least 4 decimal places recorded (the number of decimal places can be adjusted above).

This step may not be necessary for species and analyses that do not require landcover, distance to water, or similar spatial covariates. 

```{r remove_low_prec_locs, eval=FALSE}
list_removelowpreclocs <- remove_low_precision_lat_long(list_outsideactivedates, 
                                                        coordinate_precision=coord_prec)
```


## Merge close locations

Often, volunteers will maintain a camera then move it to another location very close by, and it may be advantageous later on to combine these locations into a single camera site. `merge_nearby_cameras()` does this for us and creates a new camera location variable called `cam_site_id` that can contain one or more nearby `camera_location_seq_nos`. Then, `cam_site_id` is added to effort and detections. 

Up to this point, locations are kept track of using `camera_location_seq_no`. The `camera_location_seq_no` does not indicate a unique camera location. When there is a new camera deployed or a deployment event created, even at the exact same coordinates, a new `camera_location_seq_no` is created. Also, there are cases when a new camera is deployed, sometimes at the exact same spot, but there are new coordinates associated with that deployment. This step merges the `camera_location_seq_nos` within 100 m of each other together as the same `cam_site_id`. 

For example, `camera_location_seq_no` 27962 and `camera_location_seq_no` 79626 are in very close proximity to one another. To determine whether they should be combined into a single site, we buffer each `camera_location_seq_no` (by `cam_distance` / 2), then--if they touch--we dissolve the buffers into a single buffer. Each buffer is given a `cam_site_id`, and `camera_location_seq_no` 27962 and 79626 share one in this case (847) as they are <100 meters apart:
```{r buffer_map, echo = FALSE, message = FALSE, warning = FALSE}

buffer_demo_sf <- 
  locs_df_camsiteid_demo %>% 
  dplyr::select(-dnr_grid_id) %>% 
  sf::st_as_sf(coords = c("lon", "lat"), crs = 4326) %>% 
  sf::st_transform(., 3071)

buffers_sf <- 
  buffer_demo_sf %>% 
  sf::st_buffer(dist = cam_distance/2) %>%
  arrange(cam_site_id) %>%
  group_by(cam_site_id) %>%
  mutate(id = dplyr::cur_group_id()) %>%
  ungroup()

buffers_union_sf <- 
  sf::st_cast(sf::st_union(buffers_sf), "POLYGON") %>% 
  sf::st_as_sf() %>% 
  dplyr::rename(geometry = x) %>% 
  dplyr::mutate(id = dplyr::row_number()) %>%
  st_join(buffers_sf %>% select(id,cam_site_id) %>% distinct()) %>%
  distinct()

buffer_map <- 
  buffers_union_sf %>%
  #filter(buffer_id == 2630) %>%
  #filter(buffer_id == 847) %>%
  filter(cam_site_id=='cam_0558') %>%
  mapview::mapview(layer.name = 'cam_site_id')

cam_map <- 
  buffer_demo_sf %>%
  filter(camera_location_seq_no %in% c(58322, 85895)) %>%
  #filter(camera_location_seq_no %in% c(27962,79626)) %>%
  mutate(camera_location_seq_no = as.factor(camera_location_seq_no)) %>%
  mapview::mapview(layer.name = 'camera_location_seq_no', zcol = 'camera_location_seq_no')

buffer_map + cam_map
```

A `camera_location_seq_no` is buffered and given a new ID (`cam_site_id`) regardless of whether there is a camera nearby or not. In some cases, multiple `camera_location_seq_nos` may be in the same exact location (for example, a camera is deployed and later replaced with a different camera).

The lat lon for `cam_site_id` are created later once effort is finalized and tranformed to daily effort for the purposes of creating a weighted average location.

This step does not result in any data loss. It does set up the data structure to look for instances of overlapping effort, next.
```{r merge_nearby_cameras, eval=FALSE}
list_camsiteid <- merge_nearby_cameras(list_removelowpreclocs, 
                                       cam_distance=cam_distance)
```

## Spatially subset camera locations (optional)

There may be times where it doesn't make sense to include data from the entire Snapshot Wisconsin camera network, and we should spatially subset camera locations. For example, we may want to only consider the Northern Forest Zone instead of the entire state of Wisconsin. This can be done using our list-based format to filter all 3 dataframes in one function by specifying the list, spatial layer, and statement by which to filter as a quoted string.  

```{r spatial_subset, eval=FALSE}
#Spatially subset camera locations
list_spatialfilter <- spatial_subset(datalist = list_camsiteid, 
                                     sf_layer = "dmus",
                                     filter_statement = "deer_management_zone == 'Northern Forest Zone'")
```

\
\


## Remove overlapping effort

The `cam_site_id` and season structure of the data provides the framework for assessing overlapping effort and removing site-years with overlapping effort. 

This step finds errors in the database and removes them. There is potentially more manual work that could be done to 'save' some effort. But, these fixes are manual and complicated with no clear blanket solution. For now, the best decision is to detect and remove site-years with overlapping effort. 

Overlapping effort can happen when a batch is associated with the wrong camera location. In these cases there may be batches that have the same `cam_site_id` and start and end active dates that overlap. Another common reason for overlapping effort is if the camera has been programmed with the wrong date. Even just 1 day different can result in batches with the same `cam_site_id` and overlapping start and end active dates. 

Batches at the same `cam_site_id` and year (site-year) are assessed for overlap in their start and end active dates. If overlap is detected, all the data for that site-year is removed. If there is no overlap detection at that `cam_site_id` over the other years, then those data are retained. 

For example, `cam_0243` shows overlapping effort in 2021, but no overlap in 2022 or 2023:

```{r overlap_df, echo = FALSE, message = FALSE, warning = FALSE}
effort_df_camsiteid_demo %>%
  filter(cam_site_id == 'cam_0243') %>%
  kableExtra::kbl() %>%
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover"))
```
\

To be safe, we need to discard `cam_site_ids` that have overlapping effort in a particular year (but keep other years of effort for these `cam_site_ids` if they show no overlap). In this example, we need to discard 2021 but keep 2022 and 2023. We can do that for this and other sites with `detect_remove_overlap()`. After running the function, we use the remaining `cam_site_id` x year  combinations in the effort data (which is now free of overlaps) to filter our location and detection data.
```{r detect_remove_overlap, eval=FALSE}
list_removeoverlap <- detect_remove_overlap(list_camsiteid)
```

# End of list based workflow for now

After this point you will have to specify dataframes contained in list to continue until package is updated again.

## Integrate classification precision

Finally, integrate the probability that a classification for a species and method is accurate. We assign a random number between 0 and 1 and keep the classification if is within is classification precision mean value. Otherwise, remove the classification. 

Because random numbers are used, the results of this will be different each time that this is run.

This is a rather crude approach to integrating classification precision. Initial assessment demonstrated that spatial and temporal trends using the entire dataset versus the dataset with filtering for classification precision with this approach were fairly consistent. 

*ASSESS: Should this step be a part of the decision support data workflow?*

**TO DO**
*Build class_prec.csv that has all species and classification methods.*
*Build function for sswids that automates this classification precision step.*

```{r class_prec, eval=FALSE}
class.prec = read_csv("class_prec.csv")

detections_df_precision = list_removeoverlap[["detections DF"]] %>%
  group_by(trigger_id) %>%
  mutate(rand_num = runif(1,0,1)) %>%
  ungroup() %>%
  right_join(class.prec,by=c("species","class_method")) %>%
  mutate(keep = ifelse(rand_num<=prec_mean,1,0)) %>%
  filter(keep==1)
```

```{r write_det_prec, eval=FALSE,echo=FALSE,message=FALSE,warning=FALSE}

#write_csv(detections_df_precision, "detections_df_precision.csv")
detections_df_precision <- read_csv('data_demo/detections_df_precision.csv')

```
\
\

# Process cleaned data into detection histories

## Calculate daily effort

Now that the effort data has been cleaned up and overlapping effort has been discarded, we want to use the start/end dates of each batch to create a sequence of dates from which we can tally effort per camera site, and to assign the appropriate sampling occasion.

For example, in 2023 `cam_0243` has two associated batches of triggers. The first batch starts on July 1 and ends on August 2. The next batch starts on August 2 and ends on August 31. These two batches may start and end before and after July 1 and August 31, but have been truncated to these dates to match the season date inputs: 

```{r effort_batches, echo = FALSE, message = FALSE, warning = FALSE}
effort_df_camsiteid_demo %>%
  filter(cam_site_id == 'cam_0243' & year == 2023) %>%
  kableExtra::kbl() %>%
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover"))
```
\

This level of overlap (e.g., the end date of the first batch is the same as the start date of the second batch) is fine (this can happen when an SD card is taken out and replaced on the same day), but we need to quantify the camera effort as 30 days in this case, not as 32 days. To do so, we create a sequence of dates from each batch start/end date, and only keep the distinct dates. This is done at the level of each `cam_site_id` x `year` combination:

```{r daily_effort, eval=FALSE}
# create new data frame that fills in dates between effort start/end dates
# now we can expand effort data using date ranges for each cam_site_id x year
# where date ranges are converted to multiple rows (one row for each active date)
effort_by_day_df <-
  sswids::effort_by_day(effort = list_removeoverlap[["effort DF"]])
```

```{r daily_effort_table, echo = FALSE, message = FALSE, warning = FALSE}
effort_by_day_demo %>%
  filter(cam_site_id == 'cam_0243' & year == 2023) %>%
  kableExtra::kbl() %>%
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover")) %>%
  kableExtra::scroll_box(width = "100%", height = "300px")
```
\

## Calculate coordinates by `cam_site_id`

We can use the number of days of effort generated above to weight each `camera_location_seq_no` and average the coordinates of nearby cameras. In cases where there is not a nearby camera, the lat/long for a `camera_location_seq_no` is used for the corresponding `cam_site_id`. If there are multiple `camera_location_seq_nos` with the same exact coordinates a weighted average is still calculated (weights don't contribute anything in this scenario).

There can be multiple rows per `cam_site_id` because of different `camera_location_seq_no` and/or `dnr_grid_id` at the same `cam_site_id`. 
```{r merge_coords, eval=FALSE}
mean_locs_df <-
  sswids::average_camera_coordinates(
    locations = list_removeoverlap[["locs DF"]],
    effort = effort_by_day_df
  )
```
\

For example, we have two `camera_location_seq_nos` inside the dissolved buffer for `cam_0558`: 58322 and 85895. Each of these has a different amount of effort (number of days, *n*):

```{r mean_locs_df, echo = FALSE, message = FALSE, warning = FALSE}
effort_by_day_demo %>%
  filter(cam_site_id == 'cam_0558') %>%
  group_by(cam_site_id, camera_location_seq_no) %>%
  tally() %>%
  kableExtra::kbl() %>%
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover"))
```

Therefore, when we take the weighted average of these two locations, we should end up with coordinates for `cam_0558` that are slightly closer to `camera_location_seq_no` 58322:

```{r avg_map, echo = FALSE, message = FALSE, warning = FALSE}

merged_loc <-
  mean_locs_demo %>%
  filter(cam_site_id == 'cam_0558') %>%
  slice(1) %>%
  select(-camera_location_seq_no) %>%
  st_as_sf(coords = c('lon', 'lat'), crs = 3071) %>%
  mutate(type = 'weighted_avg') %>%
  select(-cam_site_id)

cam_distance <- 100

buffer_demo_sf <- 
  locs_df_camsiteid_demo %>% 
  dplyr::select(-dnr_grid_id) %>% 
  sf::st_as_sf(coords = c("lon", "lat"), crs = 4326) %>% 
  sf::st_transform(., 3071)

buffers_sf <- 
  buffer_demo_sf %>% 
  sf::st_buffer(dist = cam_distance/2) %>%
  arrange(cam_site_id) %>%
  group_by(cam_site_id) %>%
  mutate(id = dplyr::cur_group_id()) %>%
  ungroup()

buffers_union_sf <- 
  sf::st_cast(sf::st_union(buffers_sf), "POLYGON") %>% 
  sf::st_as_sf() %>% 
  dplyr::rename(geometry = x) %>% 
  dplyr::mutate(id = dplyr::row_number()) %>%
  st_join(buffers_sf %>% select(id,cam_site_id) %>% distinct()) %>%
  distinct()

buffer_map <- 
  buffers_union_sf %>%
  #filter(buffer_id == 2630) %>%
  #filter(buffer_id == 847) %>%
  filter(cam_site_id=='cam_0558') %>%
  mapview::mapview(layer.name = 'cam_site_id')

# 27962 and 79626
cam_map <- 
  buffer_demo_sf %>%
  filter(camera_location_seq_no %in% c(58322, 85895)) %>%
  mutate(camera_location_seq_no = as.factor(camera_location_seq_no)) %>%
  #filter(camera_location_seq_no %in% c(27962, 79626)) %>%
  rename(type = camera_location_seq_no) %>%
  bind_rows(., merged_loc) %>%
  mapview::mapview(layer.name = 'type', zcol = 'type')

buffer_map + cam_map



```

Camera site coordinates are now calculated for each `cam_site_id` and the `camera_location_seq_nos` associated with it, and can be joined to the effort data later to extract environmental variables.
\

## Add in spatial covariates

```{r list_layers, eval = FALSE}
# see available spatial layers
list_spatial_layers()
```

```{r view_layers, echo = FALSE, message = FALSE, warning = FALSE}
# see available spatial layers
tibble::tribble(~layer_name, ~description, ~type, "dmus", 
                "deer management units and zones", "shapefile", "counties", 
                "county boundaries", "shapefile", "ecological_landscapes", 
                "ecological landscapes", "shapefile", "turkey_mgt_zones", 
                "turkey hunting management zones", "shapefile", "furbearer_zones", 
                "north/south trapping zones", "shapefile", "wolf_zones", 
                "wolf hunting management zones", "shapefile", "ruffed_grouse_priority_areas", 
                "ruffed grouse priority areas", "shapefile", "bear_zones", 
                "bear hunting management zones", "shapefile", "major_roads", 
                "major roads", "shapefile", "county_local_roads", "county and local roads", 
                "shapefile", "streams", "streams and rivers", "shapefile", 
                "open_water", "open water bodies", "shapefile", "wiscland2", 
                "wiscland land cover", "raster", "nlcd", "national land cover database", 
                "raster") %>% 
  dplyr::arrange(type, layer_name) %>%
  kableExtra::kbl() %>%
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover"))
```

Here, we take the `cam_site_id` locations and append in the deer management unit and zone.

Code to calculate the distance to water for each point and the proportion of landcover within different buffers of each point can be found in the additional resources section. 

These are just examples for the type of spatial data that may be included in analyses. It is expected that this code is customized to your species and questions of interest. 

**TO DO**
*Find better county layer (and replace county layer currently in sswids) that does not result in NAs for locations in Door and Ashland counties.*
*Create functions for some of the more common spatial covariate data pulls. Landcover within some buffer, distance to water.*
*Create function to bring in covariates from deployment data, like distance to trail, type of trail, height above ground, aspect of camera.*
*Fix DMU layer, wolf layer, and maybe others to not need the `st_make_valid()` function.*

```{r extract_point_data, eval=FALSE}

mean_locs_sf_layers = mean_locs_df %>%
   st_as_sf(coords = c('lon', 'lat'), crs = 3071)

# NOTE THAT COUNTY LAYER PRODUCES SOME NAS
# mean_locs_sf_layers <-
#   st_join(
#     mean_locs_sf_layers,
#     get_spatial_data("counties") %>%
#       st_transform(., st_crs(mean_locs_sf_layers)),
# 
#     join = st_within
#   )

# append the dmu layer attributes to each point
# note that st_make_valid() is needed for the dmu layer.
mean_locs_sf_layers <-
  st_join(
    mean_locs_sf_layers,
    get_spatial_data("dmus") %>%
      st_transform(., st_crs(mean_locs_sf_layers)) %>%
      st_make_valid(.),

    join = st_within
  )

```

```{r plot_dmz, echo = FALSE, message = FALSE, warning = FALSE}

mean_locs_sf_layers_demo %>%
  mapview(zcol = 'deer_management_zone', na.color = "red", color=NULL,layer.name = 'DMZ', cex = 2)

```
/

## Create sampling occasions

Next, we reshape the effort data based on the number of occasions and the daily effort at each location. We calculate the number of days active for each cam_site_id, year, and occasion. If there was no effort, the days active is entered as 0. 

**TO DO:** *Build function to use `cut` instead of `ntile` or find other ways to specify how to handle unequal intervals.*
*Create some more examples and gain a better understanding of how `ntile` allocates the extra days into certain occasions. The documentation says that the additional days are added to the first interval(s), but that is not what we see in this example below.*

```{r sampling_occasions, eval=FALSE}

# set desired number of sampling occasions
 num_occasions <- 12
# 
effort_by_occ_df <-
  sswids::create_sampling_occasions(
    seasons = seasons_df,
    effort_by_day = effort_by_day_df,
    num_occasions = num_occasions
  )

```
\

Ideally, the number of days in the season divides evenly into the number of occasions. If not, then the additional sampling days are added to one of the occasions, though this is not consistently the same occasion across years. The `sswids::create_sampling_occasions` function uses `dplyr::ntile`. Here is an example of how `ntile` allocates the additional day to different occasions by year. 

```{r ntile_example, echo=FALSE,message=FALSE,warning=FALSE}

num_occasions <- 12

day_occasion_df_table <- seasons_df %>% dplyr::group_by(year) %>% 
    tidyr::nest() %>% dplyr::mutate(date = purrr::map(data, 
    date_sequence)) %>% tidyr::unnest(date) %>% dplyr::select(-data) %>% 
    dplyr::mutate(day_of_season = dplyr::row_number()) %>% 
    dplyr::ungroup() %>% dplyr::mutate(occ = dplyr::ntile(day_of_season, 
    num_occasions)) %>%
  group_by(year,occ) %>%
  count() %>%
  pivot_wider(names_from=occ,values_from=n)

day_occasion_df_table %>%
  kableExtra::kbl(caption="Number of days by year and occasion") %>%
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover"))

```
\

Here are the first 1000 rows of `effort_by_occ_df`:

```{r effort_by_occ_demo, echo=FALSE,message=FALSE,warning=FALSE}

effort_by_occ_demo[1:1000,1:4] %>%
  kableExtra::kbl(caption="Number of days by year and occasion") %>%
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover")) %>%
  kableExtra::scroll_box(width = "100%", height = "300px")

```
\

## Calculate proportion classified by occasion

For each `cam_site_id`, year, and occasion (hereafter, `cam_site_id-year-occasion`), we query the classification table for the total number of triggers classified and the total number of triggers. We remove the unknown classifications from Zooniverse and Auto Classification. Otherwise, we do not use a cutoff of classification precision and instead include all classifications regardless of their acceptance into final data. 

We calculate proportion classified as the number of triggers classified divided by the total number of triggers for each `cam_site_id-year-occasion`. If there are no triggers in a `cam_site_id-year-occasion`, the proportion classified is assumed to be 1. At this stage, the proportion classified in occasions with 0 triggers is assumed to be 1 even if there are 0 days active. 

**TO DO:** 
*Consider whether there are updates that can be made here to improve speed of this function.*
```{r prop_class, eval=FALSE}

# use all data by setting proportion classified to 0
 ppn_class_threshold = 0
effort_by_occ_df <-
    calculate_prop_classified(
      seasons = seasons_df,
      effort_day = effort_by_day_df,
      effort_occ = effort_by_occ_df
  )

```
\

Here are the first 1000 rows of `effort_by_occ_df` now that the proportion classified information has been added:

```{r effort_by_occ_demo2, echo=FALSE,message=FALSE,warning=FALSE}

effort_by_occ_demo[1:1000,] %>%
  kableExtra::kbl() %>%
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover")) %>%
  kableExtra::scroll_box(width = "100%", height = "300px")

```
\

## Create detection histories

Now the effort data is summarized by `cam_site_id-year-occasions`. We summarize the detection data in this same way, by `cam_site_id-year-occasions`. 

There are various ways we can do this.  For example, for `DEER_ADULT_ANTLERLESS_AMT`, the total triggers for cam_0001 in 2019 occasion 1 is 7. In this row of data there were 7 triggers that were classified as `DEER_ADULT_ANTLERLESS_AMT`. This structuring of the data makes the total counts of `CLASS_KEYS` more comparable to raw data. But is also possible to obtain the maximum count of every `CLASS_KEY` per `cam_site_id-year-occasion`.  For example, for `DEER_ADULT_ANTLERLESS_AMT`, the max count for cam_0001 in 2019 occasion 1 is 1. The max count is 0 if there are any days of effort but 0 detections. The max count is NA if there are 0 days active. 

Getting either a total number of triggers or max count per `cam_site_id-year-occasions` is done by specifying the `summary_value` argument of the `sswids::summarize_detections` function to either `"count triggers"` or `"max count"`. The default is total number of triggers or `count triggers` of every `CLASS_KEY` per `cam_site_id-year-occasion`.


```{r detection_histories, eval=FALSE}

#the max count across detections within a sampling occasion
detections_wide_df_maxcount <-
  sswids::summarize_detections(
    detections = detections_df_precision,
    seasons = seasons_df,
    summary_value = "max count"
  )


# the sum of triggers with spp present across detections within a sampling occasion
# this is using the function above
detections_wide_df_counttriggers <-
  summarize_detections(
    detections = detections_df_precision,
    seasons = seasons_df,
    summary_value = "count triggers"
  )

```
\

Here are the first 1000 rows of `detections_wide_df_maxcount`:

```{r det_maxcount_tab, echo=FALSE,message=FALSE,warning=FALSE}

detections_maxcount_demo[1:1000,] %>%
  kableExtra::kbl() %>%
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover")) %>%
  kableExtra::scroll_box(width = "100%", height = "300px")

```
\

Here are the first 1000 rows of `detections_wide_df_counttriggers`:

```{r det_counttrigs_tab, echo=FALSE,message=FALSE,warning=FALSE}

detections_counttriggers_demo[1:1000,] %>%
  kableExtra::kbl() %>%
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover")) %>%
  kableExtra::scroll_box(width = "100%", height = "300px")

```
\

## Join detections, effort, and spatial information

In a final step, we join the detection count triggers and max counts `cam_site_id-year-occasion` data frame to effort. Finally, we join the columns with the coordinates and spatial covariates. 

**TO DO:** 
*`sswids::write_encounter_histories` can likely be depreciated.*
*Write function to save final .Rds files into `data_clean` folder.*

```{r final_data, eval=FALSE}

effort_by_occ_df_maxcount <-
  sswids::join_detections_effort(
    effort_by_occ = effort_by_occ_df,
    detections = detections_wide_df_maxcount,
    discard_na_rows = FALSE
  )

effort_by_occ_df_counttriggers <-
  sswids::join_detections_effort(
    effort_by_occ = effort_by_occ_df,
    detections = detections_wide_df_counttriggers,
    discard_na_rows = FALSE
  )

# merge spatial information with effort wide form 
effort_by_occ_df_maxcount_join = effort_by_occ_df_maxcount %>%
 full_join(mean_locs_sf_layers %>%
             select(-c(camera_location_seq_no,dnr_grid_id)) %>%
             distinct, .)

effort_by_occ_df_counttriggers_join = effort_by_occ_df_counttriggers %>%
 full_join(mean_locs_sf_layers %>%
             select(-c(camera_location_seq_no,dnr_grid_id)) %>%
             distinct, .)

# save effort join files
write_rds(effort_by_occ_df_maxcount_join,"data_clean/effort_by_occ_df_maxcount_join.Rds")
write_rds(effort_by_occ_df_counttriggers_join,"data_clean/effort_by_occ_df_counttriggers_join.Rds")

```
\

Here are the first 1000 rows of `effort_by_occ_df_maxcount`:

```{r eff_maxcount_tab, echo=FALSE,message=FALSE,warning=FALSE}

effort_maxcount[1:1000,] %>%
  kableExtra::kbl() %>%
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover")) %>%
  kableExtra::scroll_box(width = "100%", height = "300px")

```
\

Here are the first 1000 rows of `effort_by_occ_df_counttriggers`:

```{r eff_counttrigs_tab, echo=FALSE,message=FALSE,warning=FALSE}

effort_counttriggers[1:1000,] %>%
  kableExtra::kbl() %>%
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover")) %>%
  kableExtra::scroll_box(width = "100%", height = "300px")

```
\
\

# Data pull assessments

In the data pull assessments we first look at data losses

## Data loss summary

Here we calculate the data loss in detections, effort, and locations at each step of data cleaning. Most of the data cleaning was in part 2 of this workflow, but we summarize here to also include data loss due to low classification effort, assuming that we remove occasions where the proportion of triggers classified was < 95%.  

**TO DO**
*Create a function to produce some data loss summary graphic/table.*

```{r datalosssummary, eval=FALSE}

dets_orig = detections_df_raw %>%
  group_by(class_key) %>%
  count() %>%
  rename("total" = "n")
dets_noloc = list_nolocs[["detections DF"]] %>%
  group_by(class_key) %>%
  count() %>%
  rename("n-no_loc" = "n")
dets_baddate = list_outsideactivedates[["detections DF"]] %>%
  group_by(class_key) %>%
  count() %>%
  rename("n-bad_date" = "n")
dets_lowlocprec = list_removelowpreclocs[["detections DF"]] %>%
  group_by(class_key) %>%
  count() %>%
  rename("n-low_loc_prec" = "n")
dets_effortoverlap = list_removeoverlap[["detections DF"]] %>%
  group_by(class_key) %>%
  count() %>%
  rename("n-effort_overlap" = "n")
dets_precision = list_precision[["detections DF"]] %>%
  group_by(class_key) %>%
  count() %>%
  rename("n-class-precision" = "n")

dets_lowclasseffort = effort_by_occ_df_counttriggers %>%
  filter(ppn_classified >= 0.95) %>%
  filter(days_active > 0)
dets_lowclasseffort = colSums(dets_lowclasseffort[,9:dim(dets_lowclasseffort)[2]])
dets_lowclasseffort = as.data.frame(dets_lowclasseffort) %>%
  mutate(class_key = rownames(.)) %>%
  rename("n-low_class_effort" = "dets_lowclasseffort")

dets_all = dets_orig %>%
  left_join(dets_noloc) %>%
  left_join(dets_baddate) %>%
  left_join(dets_lowlocprec) %>%
  left_join(dets_effortoverlap) %>%
  left_join(dets_precision) %>%
  left_join(dets_lowclasseffort)

dets_all2 = dets_all %>%
  mutate(keep = `n-low_class_effort`,
         low_class_effort = `n-class-precision`-`n-low_class_effort`,
         class_precision = `n-effort_overlap` - `n-class-precision`,
         effort_overlap = `n-low_loc_prec` - `n-effort_overlap`,
         low_loc_prec = `n-bad_date` - `n-low_loc_prec`,
         bad_date = `n-no_loc` - `n-bad_date`,
         no_location = total - `n-no_loc`) %>%
  select(class_key,
         keep, 
         low_class_effort, class_precision, effort_overlap, 
         low_loc_prec, bad_date, no_location) %>%
  pivot_longer(cols=!class_key)

dets_all_plot = dets_all2 %>%
  filter(value>0) %>%
  rowwise() %>%
  mutate(value = list(seq(1, value))) %>%
  ungroup() %>%
  unnest() %>%
  select(-value) %>%
  rename("status" = "name")

dets_all_plot$status = factor(dets_all_plot$status, 
                               levels = c("keep",
                                          "no_location",
                                          "bad_date",
                                          "low_loc_prec",
                                          "effort_overlap",
                                          "class_precision",
                                          "low_class_effort"))


totals = dets_all_plot %>%
  group_by(class_key) %>%
  count() %>%
  rename(total=n)

dataloss.table = dets_all_plot %>%
  group_by(class_key,status) %>%
  count() %>%
  left_join(totals) %>%
  mutate(ppn = n/total)

dataloss.table %>%
  kableExtra::kbl() %>%
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover")) %>%
  kableExtra::scroll_box(width = "100%", height = "300px")

ggplot(dets_all_plot, aes(x=class_key,fill=status)) +
  geom_bar(position="fill") + 
  scale_fill_manual(values=c('gray','darkgreen','lightpink','#2596BE', '#8225BE','darkblue', '#D4C443')) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  labs(y="proportion",title="classification proportions by status and class_key")


```
\

This table shows the number of detections in each `CLASS_KEY` that are lost at each step as well as the triggers remaining (`keep`). 

```{r data_loss_table, echo=FALSE,message=FALSE,warning=FALSE}

totals = dets_all_plot_demo %>%
  group_by(class_key) %>%
  count() %>%
  rename(total=n)

dataloss.table = dets_all_plot_demo %>%
  group_by(class_key,status) %>%
  count() %>%
  left_join(totals) %>%
  mutate(ppn = n/total)

dataloss.table %>%
  kableExtra::kbl() %>%
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover")) %>%
  kableExtra::scroll_box(width = "100%", height = "300px")

```
\

And, a graphic to show the proportion of these data losses by `CLASS_KEY`:

```{r data_loss_graph, echo=FALSE,message=FALSE,warning=FALSE}

dets_all_plot_demo$status = factor(dets_all_plot_demo$status, 
                               levels = c("keep",
                                          "no_location",
                                          "bad_date",
                                          "low_loc_prec",
                                          "effort_overlap",
                                          "class_precision",
                                          "low_class_effort"))

ggplot(dets_all_plot_demo, aes(x=class_key,fill=status)) +
  geom_bar(position="fill") + 
  scale_fill_manual(values=c('gray','darkgreen','lightpink','#2596BE', '#8225BE','darkblue', '#D4C443')) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  labs(y="proportion",title="classification proportions by status and class_key")

```
\

## Graphs and plots of the clean data

I focus on using the count triggers detection histories when creating the following graphs. Work on this more. Jay has some useful functions for data checking that should be revisited and incorporated here. 

In below, 2023 does have an incredible increase in occ 12 as compared to previous occasions. This is worth looking into. 

**TO DO:** 
*Create functions for `sswids` to make these and other plots. Right now the code is specific to this deer example and will need to be generalized. Improve on these plots as well.*

```{r activity_plot, eval=FALSE}

activity_plot = detections_wide_df_counttriggers %>%
  group_by(year,occ) %>%
  summarise(deer_antlerless = sum(DEER_ADULT_ANTLERLESS_AMT),
            deer_antlered = sum(DEER_ADULT_ANTLERED_AMT),
            deer_unknown = sum(DEER_ADULT_UNKNOWN_AMT),
            deer_fawn = sum(DEER_FAWN_AMT)) %>%
  pivot_longer(cols=c(deer_antlerless,deer_antlered,deer_unknown,deer_fawn))

activity_plot2 = detections_wide_df_counttriggers %>%
  group_by(occ) %>%
  summarise(deer_antlerless = sum(DEER_ADULT_ANTLERLESS_AMT),
            deer_antlered = sum(DEER_ADULT_ANTLERED_AMT),
            deer_unknown = sum(DEER_ADULT_UNKNOWN_AMT),
            deer_fawn = sum(DEER_FAWN_AMT)) %>%
  pivot_longer(cols=c(deer_antlerless,deer_antlered,deer_unknown,deer_fawn))


ggplot(activity_plot, aes(x=occ,y=value,col=name)) +
  facet_wrap(~year) +
  geom_point() +
  geom_line()

ggplot(activity_plot2, aes(x=occ,y=value,col=name)) +
  geom_point() +
  geom_line()


```


```{r activity_plot_demo, echo=FALSE,eval=TRUE,message=FALSE,warning=FALSE}

activity_plot = detections_counttriggers_demo %>%
  group_by(year,occ) %>%
  summarise(deer_antlerless = sum(DEER_ADULT_ANTLERLESS_AMT),
            deer_antlered = sum(DEER_ADULT_ANTLERED_AMT),
            deer_unknown = sum(DEER_ADULT_UNKNOWN_AMT),
            deer_fawn = sum(DEER_FAWN_AMT)) %>%
  pivot_longer(cols=c(deer_antlerless,deer_antlered,deer_unknown,deer_fawn))

activity_plot2 = detections_counttriggers_demo %>%
  group_by(occ) %>%
  summarise(deer_antlerless = sum(DEER_ADULT_ANTLERLESS_AMT),
            deer_antlered = sum(DEER_ADULT_ANTLERED_AMT),
            deer_unknown = sum(DEER_ADULT_UNKNOWN_AMT),
            deer_fawn = sum(DEER_FAWN_AMT)) %>%
  pivot_longer(cols=c(deer_antlerless,deer_antlered,deer_unknown,deer_fawn))


ggplot(activity_plot, aes(x=occ,y=value,col=name)) +
  facet_wrap(~year) +
  geom_point() +
  geom_line()

ggplot(activity_plot2, aes(x=occ,y=value,col=name)) +
  geom_point() +
  geom_line()

```
\
\

# Additional resources

## Filter locations based on spatial layer

There may be times where it doesn't make sense to include data from the entire Snapshot Wisconsin camera network, and we should spatially subset camera locations. For an example here, we'll subset camera locations, detections, and effort to include only the Northern Forest Zone. We can find this layer using `list_spatial_layers()`, but layers available outside `sswids` can also be used. 

Because this layer is available in `sswids`, we can pull it in to R using `get_spatial_data()` using the layer name in the table above, and use it to remove some of the camera locations.

```{r dmu_layer, eval = FALSE}
# load an sf polygon object for subsetting
subset_layer <-
  sswids::get_spatial_data(layer_name = 'dmus') %>%
  filter(deer_management_zone=="Northern Forest Zone")
```

Here are the deer management units/zones and camera locations, prior to subsetting:

```{r pr_areas_map, echo = FALSE, message = FALSE, warning = FALSE}
layer_map <- 
  sswids::get_spatial_data(layer_name = 'dmus') %>%
  mapview(zcol = 'DEER_MANAG', layer.name = 'Deer Management Zones')

cam_map <- 
  locs_df_removelowpreclocs_demo %>%
  st_as_sf(coords = c('lon', 'lat'), crs = 3071) %>%
  mapview(cex = 1, layer.name = 'cam_site_id')

layer_map + cam_map
```

Camera locations are subset with `spatial_subset()` by supplying the averaged camera coordinates and an `sf` polygon layer.

```{r subset_layer, eval = FALSE}
# subset locations
locs_df_spatialfilter <-
  sswids::spatial_subset(
    locations = locs_df_removelowpreclocs,
    sf_layer = subset_layer
  )
```

Here are the camera locations after subsetting:

```{r subset_map, eval=FALSE,echo = FALSE, message = FALSE, warning = FALSE}

# NEEDS WORK - spatial subset function relies on cam_site_id, which is not created yet in the workflow. I think it makes sense to move this spatial filtering up in the workflow, so need to change this function.
subset_layer <-
  sswids::get_spatial_data(layer_name = 'dmus') %>%
  filter(deer_management_zone == "Northern Forest Zone")

# subset locations
# TO DO - this spatial subset function is using cam_site_id
locs_df_spatialfilter <-
  sswids::spatial_subset(
    locations = locs_df_removelowpreclocs_demo,
    sf_layer = subset_layer
  )

cam_map_2 <- 
  locs_df_spatialfilter %>%
  st_as_sf(coords = c('lon', 'lat'), crs = 3071) %>%
  mapview(cex = 1, layer.name = 'camera_location_seq_no')

layer_map + cam_map_2
```

Then we can use the remaining `cam_site_ids` to again filter the detections and effort.

```{r sub_filter, eval = FALSE}
# retain these sites in the detections and effort
detections_df_spatialfilter <-
  detections_df_removelowpreclocs %>%
  filter(cam_site_id %in% unique(locs_df_spatialfilter$cam_site_id))

effort_df_spatialfilter <-
  effort_df_removelowpreclocs %>%
  filter(cam_site_id %in% unique(locs_df_spatialfilter$cam_site_id))
```
\

## Calculate distance to water

This code from Jay calculates the distance from each location to the nearest stream or lake. It takes some time to run. 

```{r spatial_covariates, eval=FALSE}

list_spatial_layers()

mean_locs_sf_layers = mean_locs_df %>%
  st_as_sf(coords = c('lon', 'lat'), crs = 3071)

# extract distance to water - both lakes and streams/rivers
# CODE FROM JAY'S FILES

# stream distances
nearest_stream <-
  st_nn(
    mean_locs_sf_layers,
    get_spatial_data("streams") %>%
      st_transform(., st_crs(mean_locs_sf_layers)),
    #streams %>% st_transform(., 3071),
    returnDist = TRUE,
    progress = FALSE,
    parallel = 4
  )

# clean up

# function to clean up st_nn output
nn_to_df <- function(nn_out) {

  out <-
    map(
      names(nn_out),
      ~enframe(
        pluck(nn_out, .),
        name = "row_id",
        value = .
      ) %>%
        unnest(cols = all_of(.x)))

  bind_cols(out[[1]], out[[2]] %>% select(-row_id))

}

stream_distance_df <-
  nn_to_df(nn_out = nearest_stream) %>%
  rename(stream_dist = dist)

# open water distances
nearest_open_water <-
  st_nn(
    mean_locs_sf_layers,
    get_spatial_data("streams") %>%
      st_transform(., st_crs(mean_locs_sf_layers)),
    #open_water %>% st_transform(., 3071),
    returnDist = TRUE,
    progress = FALSE,
    parallel = 4
  )

# clean up
open_water_distance_df <-
  nn_to_df(nn_out = nearest_open_water) %>%
  rename(open_water_dist = dist)

# whichever distance is closest, stream or open water
distance_df <-
  stream_distance_df %>%
  bind_cols(., open_water_distance_df %>% select(open_water_dist)) %>%
  rowwise() %>%
  # take min of either column
  mutate(dist = min(stream_dist, open_water_dist))

```
\

## Calculate proportion of landcover within buffered distance of points

Use WISCLAND to calculate the proportion of landcover within some distance buffer of each point. You can select different levels of WISCLAND.

```{r wiscland_level1, echo=FALSE,message=FALSE,warning=FALSE}

wiscland_level1 = read_csv("data_demo/Wiscland2_Key_Level1.csv")

wiscland_level1 %>%
  kableExtra::kbl(caption = "WISCLAND 2.0 Level 1") %>%
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover"),
                            full_width = FALSE,
                            position = "left")

```
\

```{r wiscland_level2, echo=FALSE,message=FALSE,warning=FALSE}

wiscland_level2 = read_csv("data_demo/Wiscland2_Key_Level2.csv")

wiscland_level2 %>%
  kableExtra::kbl(caption = "WISCLAND 2.0 Level 2") %>%
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover"),
                            full_width = FALSE,
                            position = "left")

```
\

```{r extract_landcover, eval=FALSE}

# convert mean_locs_sf projection to wiscland2 projection
mean_locs_sf_layers <- 
  mean_locs_sf_layers %>%
  # use wiscland crs
  st_transform(., st_crs(sswids::get_spatial_data(layer_name = 'wiscland2', level = 1)))

# now extract data within buffer(s) surrounding camera locations
# this can be done for several buffers at a time for multi-scale analyses
buffers <- c(10,100,500,1000) # buffer sizes in meters

# calculate 'pland' for each camera x buffer combination
# pland = proportion of landscape in FRAGSTATS
lm_output <- 
  buffers %>% 
  set_names() %>% 
  # produce a dataframe after this is all done
  map_dfr( 
    ~sample_lsm(
      # raster layer
      landscape = sswids::get_spatial_data(layer_name = 'wiscland2', level = 1),
      # camera locations
      y = mean_locs_sf_layers,
      # get landcover class level metrics
      level = "class",
      # return NA values for classes not in buffer
      # all_classes = TRUE, 
      # camera site IDs here
      plot_id = mean_locs_sf_layers$cam_site_id,
      # can do multiple metrics at once
      what = 'lsm_c_pland',
      # buffer sizes to use
      size = ., 
      # default is square buffer
      shape = "circle", 
      # turn warnings on or off
      verbose = FALSE 
    ), 
    # get buffer size column in the output
    .id = "buffer_size"
  )

# in this data frame plot_id = camera ID
# class = landcover type
# value = % of that landcover type in the buffer
# make each landcover type x buffer into a column
lm_output <- 
  lm_output %>%
  # MAY NEED TO ADD distinct() HERE???
  pivot_wider(
    id_cols = plot_id,
    names_from = c(class, buffer_size),
    values_from = c(value),
    # give class 0 if it doesn't exist in buffer
    values_fill = 0
  ) %>%
  # clean up names
  rename(cam_site_id = plot_id)

# join pland back to camera sf object
mean_locs_sf_layers <- 
  mean_locs_sf_layers %>%
  left_join(., lm_output)


```
\

## `sswids` READ ME from Jay

R package for the [Snapshot Wisconsin](https://dnr.wisconsin.gov/topic/research/projects/snapshot) Decision Support Team

### Installation

You can install the development version of sswids from [GitHub](https://github.com/) with:

```{r install_sswids, eval=FALSE}

# install.packages("devtools")
devtools::install_github("SnapshotWisconsin/sswids")

```

### Clone repository

- On GitHub repo, go to Code tab and copy URL
- Open RStudio > File > New Project > Version Control > Git
- Paste repo URL and browse to directory where you want to clone the package on your computer
- Click on `.proj` file to open project in RStudio in the future

### Add/modify functions

- Load `devtools`
- Create new function: `use_r('new_function_name')`
- Write function in new R script that pops up
- Click somewhere inside function, then go to Code > Insert Roxygen Skeleton to add documentation
- Save R script
- Check the function is working, use `load_all()` to do tests with function right away
- Run `check()` to make sure there are no errors
- Run `document()` to make sure documentation is updated

### Commit/push to Github

- Click on `Git` tab
- Click `Staged` box next to new or modified file
- Click `Commit`
- Enter commit message
- Click `Push`

Then update R package by first removing with `remove.packages()` or `devtools::install_github("SnapshotWisconsin/sswids", force = TRUE)`

### Update spatial data

- Open data-raw/make_spatial_data.R
- Grab DNR GIS files using arcpullr package or drop shapefiles into data-raw
- Copy OAS/Winiarski/snapshot/sswids_gis into C:/sswids_gis to access large shapefiles/rasters
